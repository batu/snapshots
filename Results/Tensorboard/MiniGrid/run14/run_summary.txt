Experiment name: MiniGrid 
Save prob:0.0 
Load prob:0.0 

NUM_CPU = 1
ENV_NAME = "MiniGrid-Empty-16x16-v0"
human_snapshots = False
training_length = 40000
snapshot_usage_percentage = 1
save_prob, load_prob, experiment_name = parseArguments()

# Create an OpenAIgym environment.
#env = make_atari_snapshot_env(ENV_NAME, num_env=NUM_CPU , seed=37, snapshot_save_prob=0.001, snapshot_load_prob=0.75)

# env = SnapshotVecEnv([make_env(i) for i in range(NUM_CPU)],
#                        snapshot_save_prob=save_prob,
#                        snapshot_load_prob=load_prob,
#                        human_snapshots=True,
#                        training_len=40000)

env = gym.make(ENV_NAME)
# env = ActionBonus(env)
# env = StateBonus(env)
env = FlatObsWrapper(env)
env = Monitor(env, filename=run_path, allow_early_resets=True)
env = DummyVecEnv([lambda: env for i in range(NUM_CPU)],
                       # snapshot_save_prob=save_prob,
                       # snapshot_load_prob=load_prob,
                       # human_snapshots=human_snapshots,
                       # training_len=training_length,
                       # snapshot_usage_percentage=snapshot_usage_percentage
                       )

# env = Monitor(env, filename=run_path, allow_early_resets=True)
# Frame-stacking with 4 frames
# env = VecFrameStack(env, n_stack=4)

# Add some param noise for exploration
model = PPO2(MlpPolicy, env, verbose=1, tensorboard_log=f"{run_path}",
     gamma=0.999,
     lam=.95,
     vf_coef=1,
     ent_coef=0.01,
     noptepochs=3,
     cliprange=lambda f: f * 0.1,
     learning_rate=lambda f: f * 2.5e-4,
     nminibatches=4,
     n_steps=128,
    )

# model = PPO2(CnnPolicy, env, verbose=1, tensorboard_log=f"{run_path}",
#      gamma=0.99,
#      lam=.95,
#      vf_coef=1,
#      ent_coef=0.01,
#      noptepochs=3,
#      cliprange=lambda f: f * 0.1,
#      learning_rate=lambda f: f * 2.5e-4,
#      nminibatches=4,
#      n_steps=128,
#      )

# model = DQN(DQNMlpPolicy, env, verbose=1)

best_mean_reward, n_steps = -np.inf, 0
last_name = "dummy"
def callback(_locals, _globals):
  """
  Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)
  :param _locals: (dict)
  :param _globals: (dict)
  """
  global n_steps, best_mean_reward, last_name
  # Print stats every 1000 calls

  if (n_steps + 1) % 100 == 0:
      # Evaluate policy performance
      x, y = ts2xy(load_results(run_path), 'timesteps')
      if len(x) > 0:
          mean_reward = np.mean(y[-100:])
          print(f"Save probability: {save_prob}, load probability: {load_prob}")
          print("Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}".format(best_mean_reward, mean_reward))

          # New best model, you could save the agent here
          if mean_reward > best_mean_reward:
              best_mean_reward = mean_reward
              # Example for saving best model
              if os.path.exists(run_path + f'/{last_name}'):
                  os.remove(run_path + f'/{last_name}')
              print("Saving new best model")
              last_name = f"{best_mean_reward:.1f}_{ENV_NAME}_model.pkl"
              _locals['self'].save(run_path + f'/{best_mean_reward:.1f}_{ENV_NAME}_model.pkl')
  n_steps += 1
  return False

# Train the agent
model.learn(total_timesteps= training_length, callback=callback)
