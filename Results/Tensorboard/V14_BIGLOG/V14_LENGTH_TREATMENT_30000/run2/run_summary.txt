Experiment name: V14_LENGTH_TREATMENT_30000 
Save prob:0.0 
Load prob:1.0 

NUM_CPU = 1
ENV_NAME = "MountainCar-v0"
human_snapshots = True
training_length = 40000
snapshot_usage_percentage = 1
save_prob, load_prob, experiment_name = parseArguments()
num_runs = 20
treatments = [True, False]
for treatment in treatments:
    if treatment:
        snapshot_usage_percentage = 0.75
    else:
        snapshot_usage_percentage = 1
    experiment_name = f"{experiment_name}_TREATMENT" if treatment else f"{experiment_name}_VANILLA"
    for run_count in range(num_runs):

        # Create an OpenAIgym environment.
        #env = make_atari_snapshot_env(ENV_NAME, num_env=NUM_CPU , seed=37, snapshot_save_prob=0.001, snapshot_load_prob=0.75)

        # env = SnapshotVecEnv([make_env(i) for i in range(NUM_CPU)],
        #                        snapshot_save_prob=save_prob,
        #                        snapshot_load_prob=load_prob,
        #                        human_snapshots=True,
        #                        training_len=40000)

        env = SnapshotVecEnv([lambda:Monitor(gym.make(ENV_NAME), filename=run_path, allow_early_resets=True) for i in range(NUM_CPU)],
                               snapshot_save_prob=save_prob,
                               snapshot_load_prob=load_prob,
                               human_snapshots=human_snapshots,
                               training_len=training_length,
                               snapshot_usage_percentage=snapshot_usage_percentage)

        # env = Monitor(env, filename=run_path, allow_early_resets=True)
        # Frame-stacking with 4 frames
        # env = VecFrameStack(env, n_stack=4)

        # Add some param noise for exploration
        # model = PPO2(MlpPolicy, env, verbose=1, tensorboard_log=f"{run_path}",
        #              gamma=0.99,
        #              lam=.95,
        #              vf_coef=1,
        #              ent_coef=0.01,
        #              noptepochs=3,
        #              cliprange=0.1,
        #              learning_rate=5e-4,
        #              nminibatches=4,
        #              n_steps=128,
        #              )
        model = DQN(DQNMlpPolicy, env, verbose=1)

        best_mean_reward, n_steps = -np.inf, 0
        last_name = "dummy"
        def callback(_locals, _globals):
          """
          Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)
          :param _locals: (dict)
          :param _globals: (dict)
          """
          global n_steps, best_mean_reward, last_name
          # Print stats every 1000 calls

          if (n_steps + 1) % 100 == 0:
              # Evaluate policy performance
              x, y = ts2xy(load_results(run_path), 'timesteps')
              if len(x) > 0:
                  mean_reward = np.mean(y[-100:])
                  print(f"Save probability: {save_prob}, load probability: {load_prob}")
                  print("Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}".format(best_mean_reward, mean_reward))

                  # New best model, you could save the agent here
                  if mean_reward > best_mean_reward:
                      best_mean_reward = mean_reward
                      # Example for saving best model
                      if os.path.exists(run_path + f'/{last_name}'):
                          os.remove(run_path + f'/{last_name}')
                      print("Saving new best model")
                      last_name = f"{best_mean_reward:.1f}_{ENV_NAME}_model.pkl"
                      _locals['self'].save(run_path + f'/{best_mean_reward:.1f}_{ENV_NAME}_model.pkl')
          n_steps += 1
          return False

        # Train the agent
        model.learn(total_timesteps= training_length, callback=callback)
