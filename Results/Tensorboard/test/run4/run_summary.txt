Experiment name: test 
Save prob:0 
Load prob:0 

NUM_CPU = 1
ENV_NAME = "CartPole-v0"

# Create an OpenAIgym environment.
#env = make_atari_snapshot_env(ENV_NAME, num_env=NUM_CPU , seed=37, snapshot_save_prob=0.001, snapshot_load_prob=0.75)


env = gym.make(ENV_NAME)
env = SnapshotVecEnv([lambda: env for i in range(NUM_CPU)],
                       snapshot_save_prob=0.001,
                       snapshot_load_prob=1)

# env = SnapshotVecEnv([make_env(i) for i in range(NUM_CPU)],
#                        snapshot_save_prob=0.001,
#                        snapshot_load_prob=1)

# Frame-stacking with 4 frames
env = VecFrameStack(env, n_stack=4)

# Add some param noise for exploration
model = PPO2(MlpPolicy, env, verbose=1, tensorboard_log=f"{run_path}",
             gamma=0.99,
             lam=.95,
             vf_coef=1,
             ent_coef=0.005,
             noptepochs=3,
             cliprange=0.1,
             learning_rate=5e-4,
             nminibatches=1,
             n_steps=32,
             )

best_mean_reward, n_steps = -np.inf, 0
last_name = "dummy"
def callback(_locals, _globals):
  """
  Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)
  :param _locals: (dict)
  :param _globals: (dict)
  """
  global n_steps, best_mean_reward, last_name
  # Print stats every 1000 calls

  if (n_steps + 1) % 10 == 0:
      # Evaluate policy performance
      x, y = ts2xy(load_results(run_path), 'timesteps')
      if len(x) > 0:
          mean_reward = np.mean(y[-100:])
          print(f"Save probability: {save_prob}, load probability: {load_prob}")
          print("Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}".format(best_mean_reward, mean_reward))

          # New best model, you could save the agent here
          if mean_reward > best_mean_reward:
              best_mean_reward = mean_reward
              # Example for saving best model
              if os.path.exists(run_path + f'/{last_name}'):
                  os.remove(run_path + f'/{last_name}')
              print("Saving new best model")
              last_name = f"{best_mean_reward:.1f}_{ENV_NAME}_model.pkl"
              _locals['self'].save(run_path + f'/{best_mean_reward:.1f}_{ENV_NAME}_model.pkl')
  n_steps += 1
  return False

# Train the agent
model.learn(total_timesteps= 40000000, callback=callback)
